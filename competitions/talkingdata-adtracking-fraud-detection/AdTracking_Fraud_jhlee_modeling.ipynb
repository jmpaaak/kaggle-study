{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This version has improvements based on new feature engg techniques observed from different kernels. Below are few of them:\n",
    "- https://www.kaggle.com/graf10a/lightgbm-lb-0-9675\n",
    "- https://www.kaggle.com/rteja1113/lightgbm-with-count-features?scriptVersionId=2815638\n",
    "- https://www.kaggle.com/nuhsikander/lgbm-new-features-corrected?scriptVersionId=2852561\n",
    "- https://www.kaggle.com/aloisiodn/lgbm-starter-early-stopping-0-9539 (Original script)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "loading test data...\n",
      "Extracting new features...\n",
      "grouping by : ip-day-hour combination...\n",
      "grouping by : ip-app combination...\n",
      "grouping by : ip-app-device combination...\n",
      "grouping by : ip-app-os combination...\n",
      "grouping by : ip_day_chl_var_hour\n",
      "grouping by : ip_app_os_var_hour\n",
      "grouping by : ip_app_channel_var_day\n",
      "grouping by : ip_app_chl_mean_hour\n",
      "merging...\n",
      "vars and data type: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 203694359 entries, 0 to 203694358\n",
      "Data columns (total 18 columns):\n",
      "app                         uint16\n",
      "channel                     uint16\n",
      "click_id                    float64\n",
      "click_time                  object\n",
      "device                      uint16\n",
      "ip                          uint32\n",
      "is_attributed               float64\n",
      "os                          uint16\n",
      "hour                        uint8\n",
      "day                         uint8\n",
      "ip_tcount                   int64\n",
      "ip_app_count                int64\n",
      "ip_app_device_count         int64\n",
      "ip_app_os_count             int64\n",
      "ip_tchan_count              float64\n",
      "ip_app_os_var               float64\n",
      "ip_app_channel_var_day      float64\n",
      "ip_app_channel_mean_hour    float64\n",
      "dtypes: float64(6), int64(4), object(1), uint16(4), uint32(1), uint8(2)\n",
      "memory usage: 20.9+ GB\n",
      "train size:  175903890\n",
      "valid size:  9000000\n",
      "test size :  18790469\n",
      "Training...\n",
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\ttrain's auc: 0.956913\tvalid's auc: 0.964399\n",
      "[20]\ttrain's auc: 0.965973\tvalid's auc: 0.971936\n",
      "[30]\ttrain's auc: 0.968604\tvalid's auc: 0.973629\n",
      "[40]\ttrain's auc: 0.970315\tvalid's auc: 0.975423\n",
      "[50]\ttrain's auc: 0.971238\tvalid's auc: 0.976469\n",
      "[60]\ttrain's auc: 0.97199\tvalid's auc: 0.977489\n",
      "[70]\ttrain's auc: 0.972522\tvalid's auc: 0.977999\n",
      "[80]\ttrain's auc: 0.972877\tvalid's auc: 0.978355\n",
      "[90]\ttrain's auc: 0.973177\tvalid's auc: 0.97879\n",
      "[100]\ttrain's auc: 0.973539\tvalid's auc: 0.9791\n",
      "[110]\ttrain's auc: 0.973765\tvalid's auc: 0.979363\n",
      "[120]\ttrain's auc: 0.973983\tvalid's auc: 0.979552\n",
      "[130]\ttrain's auc: 0.974172\tvalid's auc: 0.979746\n",
      "[140]\ttrain's auc: 0.974316\tvalid's auc: 0.97977\n",
      "[150]\ttrain's auc: 0.974483\tvalid's auc: 0.979885\n",
      "[160]\ttrain's auc: 0.974611\tvalid's auc: 0.979981\n",
      "[170]\ttrain's auc: 0.974777\tvalid's auc: 0.980075\n",
      "[180]\ttrain's auc: 0.974895\tvalid's auc: 0.980031\n",
      "[190]\ttrain's auc: 0.975004\tvalid's auc: 0.980101\n",
      "[200]\ttrain's auc: 0.975123\tvalid's auc: 0.980164\n",
      "[210]\ttrain's auc: 0.975194\tvalid's auc: 0.980143\n",
      "[220]\ttrain's auc: 0.975305\tvalid's auc: 0.980226\n",
      "[230]\ttrain's auc: 0.975424\tvalid's auc: 0.980326\n",
      "[240]\ttrain's auc: 0.975537\tvalid's auc: 0.980386\n",
      "[250]\ttrain's auc: 0.975601\tvalid's auc: 0.980408\n",
      "[260]\ttrain's auc: 0.975686\tvalid's auc: 0.980396\n",
      "[270]\ttrain's auc: 0.975754\tvalid's auc: 0.980396\n",
      "[280]\ttrain's auc: 0.975831\tvalid's auc: 0.980391\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttrain's auc: 0.975656\tvalid's auc: 0.980453\n",
      "\n",
      "Model Report\n",
      "n_estimators :  255\n",
      "auc: 0.9804528703747412\n",
      "[10942.536453723907]: model training time\n",
      "Predicting...\n",
      "writing...\n",
      "done...\n",
      "1522058103.3184667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "\n",
    "    return bst1\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "print('loading train data...')\n",
    "train_df = pd.read_csv(path+\"train.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed']) # skiprows=range(1,144903891), nrows=40000000\n",
    "\n",
    "print('loading test data...')\n",
    "test_df = pd.read_csv(path+\"test.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "len_train = len(train_df)\n",
    "train_df=train_df.append(test_df)\n",
    "\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "print('Extracting new features...')\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip-day-hour combination...')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_tcount'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip-app combination...')\n",
    "gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip-app-device combination...')\n",
    "gp = train_df[['ip', 'app', 'device', 'channel']].groupby(by=['ip', 'app', 'device'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_device_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'device'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip-app-os combination...')\n",
    "gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Adding features with var and mean hour (inspired from nuhsikander's script)\n",
    "print('grouping by : ip_day_chl_var_hour')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','channel'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_tchan_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_os_var_hour')\n",
    "gp = train_df[['ip','app', 'os', 'hour']].groupby(by=['ip', 'app', 'os'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_app_os_var'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_channel_var_day')\n",
    "gp = train_df[['ip','app', 'channel', 'day']].groupby(by=['ip', 'app', 'channel'])[['day']].var().reset_index().rename(index=str, columns={'day': 'ip_app_channel_var_day'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_chl_mean_hour')\n",
    "gp = train_df[['ip','app', 'channel','hour']].groupby(by=['ip', 'app', 'channel'])[['hour']].mean().reset_index().rename(index=str, columns={'hour': 'ip_app_channel_mean_hour'})\n",
    "print(\"merging...\")\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print(\"vars and data type: \")\n",
    "train_df.info()\n",
    "train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16')\n",
    "train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')\n",
    "\n",
    "\n",
    "test_df = train_df[len_train:]\n",
    "val_df = train_df[(len_train-9000000):len_train]\n",
    "train_df = train_df[:(len_train-9000000)]\n",
    "\n",
    "print(\"train size: \", len(train_df))\n",
    "print(\"valid size: \", len(val_df))\n",
    "print(\"test size : \", len(test_df))\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'day', \n",
    "              'ip_tcount', 'ip_tchan_count', 'ip_app_count', 'ip_app_device_count',\n",
    "              'ip_app_os_count', 'ip_app_os_var',\n",
    "              'ip_app_channel_var_day','ip_app_channel_mean_hour']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 7,  # 2^max_depth - 1\n",
    "    'max_depth': 3,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':99 # because training data is extremely unbalanced \n",
    "}\n",
    "bst = lgb_modelfit_nocv(params, \n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=30, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=500, \n",
    "                        categorical_features=categorical)\n",
    "\n",
    "print('[{}]: model training time'.format(time.time() - start_time))\n",
    "del train_df\n",
    "del val_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Predicting...\")\n",
    "sub['is_attributed'] = bst.predict(test_df[predictors])\n",
    "print(\"writing...\")\n",
    "sub.to_csv('sub3.csv',index=False)\n",
    "print(\"done...\")\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
